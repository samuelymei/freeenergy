\chapter{Evaluation of Reliability\label{chapter:Eva}}
\begin{chapquote}{Albert Einstein %\textit{\url{https://en.wikiquote.org/wiki/Albert_Einstein}}
	}
	``A theory is something nobody believes, except the person who made it. An experiment is something everybody believes, except the person who made it.''
\end{chapquote}
\section{Overlap Matrix\label{Sec:Eva:OM}}
Overlap matrix proposed by Mobley et. al.,\cite{KlimovichJCAMD2015} can be used to essentially measures the magnitude of the phase space overlap. For example, after MBAR method is used to analyze the US simulations or a series of alchemical window simulations, the overlap matrix can be used to examine the reliabilities of MBAR calculations. The formula about the overlap matrix is shown as follows:

For the US simulations, with the weight of the $l$th configuration in the $i$th biased simulation appearing in the $t$th simulation defined as
\begin{align}
	w_{t}(\textbf{x}_{i,l})
	=&\frac{e^{-\beta \left[W_{t}(\textbf{x}_{i,l})-f_{t}^{(b)}\right]}}{\sum\limits_{k=1}^{S}N_{k}e^{-\beta\left[W_{k}(\textbf{x}_{i,l})-f_{k}^{(b)}\right]}},
	\label{Eq:weight3} 
\end{align}
the elements of the $S \times S$ overlap matrix are\cite{KlimovichJCAMD2015}
\begin{align}
	O_{tt^{\prime}} =&\sum\limits_{i=1}^{S}\sum\limits_{l=1}^{N_{i}}N_tw_{t}(\textbf{x}_{i,l})w_{t^\prime}(\textbf{x}_{i,l})\notag\\
	=&
	\sum\limits_{i=1}^{S}\sum\limits_{l=1}^{N_{i}}
	\frac{N_{t}e^{-\beta\left[W_{t}(\textbf{x}_{i,l})-f_{t}^{(b)}\right]}e^{-\beta \left[W_{t^{\prime}}(\textbf{x}_{i,l})-f_{t^{\prime}}^{(b)}\right]}}{\left\{\sum\limits_{k=1}^{S}N_{k}e^{-\beta\left[W_{k}(\textbf{x}_{i,l})-f_{k}^{(b)}\right]}\right\}^2}.
	\label{Eq:OM}
\end{align}
Consecutive windows should have substantial overlap with the diagonal and the first off-diagonal elements no smaller than 0.03 as recommended\cite{KlimovichJCAMD2015}. 

For a series of alchemical window simulations, it is a $K \times K$ matrix with entries
\begin{equation}
O_{ij}=\sum\limits_{n=1}^{N}\frac{N_{i}p_{i}(x_{n})}{\sum\limits_{k=1}^{K}N_{k}p_{k}(x_{n})}\cdot \frac{p_{j}(x_{n})}{\sum\limits_{k=1}^{K}N_{k}p_{k}(x_{n})},
\end{equation}
where $p_{i}(x_{n})=e^{\beta G_{i}-\beta U_{i}(x_{n})}$ is the probability of sample $x_{n}$ occurring when simulation state $i$ and $N$ samples are collected with $N_{1}$ samples from $p_{1}(x)$ distribution, $N_{2}$ samples from $p_{2}(x)$ distribution, and so on. $K$ is the total number of the states. $O_{ij}$ can be interpreted as the average probability of a sample generated in state $j$ being observed in the $i$th state. The average is computed over samples collected from all the $K$ states, not just the samples from state $j$. Therefore $O_{ij}$ is a measure of the overlap in the phase space of state $i$ and $j$. The larger the better. The largest eigenvalue is 1.
Similarly, consecutive windows should have substantial overlap with the diagonal and the first off-diagonal elements no smaller than 0.03 as recommended\cite{KlimovichJCAMD2015}. 

\section{\texorpdfstring{$\Pi$ Metric for Neglected-tail Bias Model}{Π Metric for Neglected-tail Bias Model}\label{Sec:Eva:Pi}}
The neglected-tail bias model is developed by Kofke and coworkers for the estimate of bias in free energy via thermodynamic perturbation or the Jarzynski equality\cite{LuJCP2001,WuJCP2004}. Here, we will take the latter as an example. Let $p_A(W)$ and $p_B(W)$ be the distributions/probabilities of work samples $W$ in a forward ($A\to B$) and a backward ($B\to A$) nonequilibrium conversions. The Jarzynski equality discussed in \ref{Sec:FEM:NEW} shows that the free energy difference is given by
\begin{equation}
    \exp{(-\beta \Delta A)}=\int_{-\infty}^{\infty}p_A(W)e^{-\beta W}\diff W,
\end{equation}
or
\begin{equation}
    \exp{(+\beta \Delta A)}=\int_{-\infty}^{\infty}p_B(W)e^{+\beta W}\diff W.
\end{equation}
These two distributions are related
\begin{equation}
    p_A(W)e^{-\beta W}=p_B(W)e^{-\beta \Delta A}.
    \label{Eq:Eva:Pi:distributions_and_dA}
\end{equation}
The neglected-tail bias model asserts that \emph{all} of the bias is due the neglect of contributions below a particular value of $W$, designated as $W^\ast$, such that no sampling is contributed below this value, and \emph{perfect sampling} is achieved for $W>W^\ast$.

It can be easily imagined that $W^\ast$ depends on the sample size. When more sampling is performed, more likely a more negative value of $W^\ast$ will be encountered. Given $p_A(W)$, the probability distribution for $W^\ast$ being observed once within $M$ samples is
\begin{equation}
    P_A^\ast(W^\ast)=Mp_A(W^\ast)[1-C_A(W^\ast)]^{M-1},
    \label{Eq:Eva:Pi:prob_of_Wstar}
\end{equation}
where $C_A(W)$ is the cumulative distribution function defined as
\begin{equation}
    C_A(W^\ast)=\int_{-\infty}^{W^\ast} p_A(W)\diff W
\end{equation}
and
\begin{equation}
    1-C_A(W^\ast)=\int_{W^\ast}^{\infty} p_A(W)\diff W.
\end{equation}
The bias can be written as
\begin{align}
    B(M)=&\left<\Delta A(M)\right>-\Delta A\notag\\
        =&-\beta^{-1}\ln{\left[\frac{1}{M}\left(e^{-\beta W^\ast}+(M-1)\cdot \frac{\displaystyle\int_{W^\ast}^{\infty}\diff We^{-\beta W}p_A(W)}{1-C_A(W^\ast)}\right)\right]}-
        \Delta A\notag\\
        =&-\beta^{-1}\ln{\left[\frac{1}{M}\left(e^{-\beta W^\ast_{dis}}+(M-1)\cdot \frac{1-C_B(W^\ast)}{1-C_A(W^\ast)}\right)\right]},
        \label{Eq:Eva:Pi:bias}
\end{align}
where
\begin{equation}
    W_{dis}=W-\Delta A
\end{equation}
is the dissipated work. For the third equality, we have employed Eq.~\ref{Eq:Eva:Pi:distributions_and_dA}. The bias can be evaluated as the average of the inaccuracy over the distribution of $W^\ast$
\begin{equation}
    B(M)=-\beta^{-1}\int_{-\infty}^\infty\diff W^\ast P_A^\ast(W^\ast)\ln{\left[\frac{1}{M}\left(e^{-\beta W^\ast_{dis}}+(M-1)\cdot \frac{1-C_B(W^\ast)}{1-C_A(W^\ast)}\right)\right]}.
\end{equation}
Alternatively, it can be estimated for a single value of $W^\ast$, for instance the mode $\hat{W}^\ast$ of $P_A^\ast$. It follows from  
\begin{equation}
    \frac{\diff \ln{P_A(W^\ast)}}{\diff W^\ast}\biggr\rvert_{W=\hat{W}^\ast}=0
\end{equation}
and is given by the solution of
\begin{equation}
    \frac{\diff \ln{p_A(W)}}{\diff W}\biggr\rvert_{W=\hat{W}^\ast}=\frac{(M-1)p_A(\hat{W}^\ast)}{1-C_A(\hat{W}^\ast)}.
    \label{Eq:Eva:Pi:Wmax}
\end{equation}
or, with a small approximation
\begin{equation}
    \frac{\diff \ln{p_A(W)}}{\diff W}\biggr\rvert_{W=\hat{W}^\ast}=(M-1)p_A(\hat{W}^\ast).
    \label{Eq:Eva:Pi:Wmax2}
\end{equation}
Assuming $W^\ast$ has a Gaussian distribution centered at $\hat{W}^\ast$ as
\begin{equation}
    P_A^\ast(W^\ast)\approx \frac{1}{\sqrt{2\pi}\sigma^\ast}\exp{\left[-(W^\ast-\hat{W}^\ast)^2/2{\left(\sigma^\ast\right)}^2\right]}
\end{equation}
in which, using Eq.~\ref{Eq:Eva:Pi:prob_of_Wstar} and the property in Eq.~\ref{Eq:Eva:Pi:Wmax},
\begin{equation}
    \frac{1}{{\left(\sigma^\ast\right)}^2}=-\frac{\diff^2\,\ln{P_A^\ast}}{\diff\, {\left(W^\ast\right)}^2}=-\left.\left[\frac{\partial^2 \ln{p_A}}{\partial W^2}-\frac{M}{M-1}\left(\frac{\partial \ln p_A}{\partial W}\right)^2\right]\right\rvert_{W=\hat{W}^\ast}.
\end{equation}
Corresponding relations can be given for the bias of the $B\to A$ work process. Thus from the work distributions the expected performance of a simulation of given sampling length can be predicted.

Next, let us look at the work distributions. We assume a Gaussian-work distribution
\begin{equation}
    p_A(W)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\left[-(W-\bar{W})^2/2\sigma^2\right]}.
    \label{Eq:Eva:Pi:pAW}
\end{equation}
From Eq.~\ref{Eq:Eva:Pi:distributions_and_dA}, it can be found that $p_B(W)$ is also a Gaussian with the same variance but shifted by $\beta\sigma^2$
\begin{equation}
    p_B(W)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\left[-(W-\bar{W}+\beta\sigma^2)^2/2\sigma^2\right]}.
\end{equation}
The free energy difference is
\begin{equation}
    \Delta A=\bar{W}-\beta\sigma^2.
    \label{Eq:Eva:Pi:dA}
\end{equation}
The cumulative distribution functions are
\begin{align}
    C_A(W)=&\frac{1}{2} \erfc \left(\frac{\bar{W}-W}{\sqrt{2}\sigma}\right),\\
    C_B(W)=&\frac{1}{2} \erfc\left(\frac{\bar{W}-\beta\sigma^2-W}{\sqrt{2}\sigma}\right),
\end{align}
where $\erfc(x)$ is the complementary error function.

By taking Eq.~\ref{Eq:Eva:Pi:pAW} into Eq.~\ref{Eq:Eva:Pi:Wmax2}, the mode of the least-work distribution is approximately
\begin{equation}
    \hat{W}^\ast=\bar{W}-\sigma\sqrt{\mathbf{W}_L\left[\frac{1}{2\pi}(M-1)^2\right]},
\end{equation}
where $\mathbf{W}_L(x)$ is the Lambert $W$ function, defined as the solution to $x=we^w$. With a slight approximation to Eq.~\ref{Eq:Eva:Pi:bias} as
\begin{equation}
    B(M)=-\beta^{-1}\ln{[1-C_B(W^\ast)]}
\end{equation}
and this estimated mode used for $W^\ast$, this Gaussian-work bias becomes
\begin{equation}
    B(M)=-\beta^{-1}\ln{\left\{\frac{1}{2}\erfc\left[-\frac{1}{\sqrt{2}}\left(\sqrt{\mathbf{W}_L\left[\frac{1}{2\pi}(M-1)^2\right]}-\beta\sigma\right)\right]\right\}},
\end{equation}
and depends solely on
\begin{equation}
    \Pi=\sqrt{\mathbf{W}_L\left[\frac{1}{2\pi}(M-1)^2\right]}-\beta\sigma.
\end{equation}
By using Eq.~\ref{Eq:Eva:Pi:dA}, it can be rewritten as
\begin{equation}
\Pi=\sqrt{\mathbf{W}_L\left[\frac{1}{2\pi}(M-1)^2\right]}-\sqrt{2\beta(\bar{W}-\Delta A)}.
\end{equation}
Wu and Kofke suggested that the number of work samples (non-equilibrium trajectories) should be sufficient to ensure $\Pi>0.5$.\cite{WuJCP2004} This idea has also been applied to the estimate of the bias in bridge estimators such as BAR.\cite{RadakJCP2019}

\section{Kullback–Leibler divergence\label{Sec:Eva:KLDverg}}
The convergence rate of thermodynamic perturbation heavily depends on the similarity between the simulated Hamiltonian ($A$) and the target one ($B$). Kullback–Leibler divergence offers a quantitative way to measure the similarity by
\begin{align}
	KL(p_A||p_B)=&\int_\Gamma \diff \mathrm{x}\, p_A(\mathbf{x})\ln\left[\frac{p_A(\mathbf{x})}{p_B(\mathbf{x})}\right]\notag\\
	   =&-\left(-\int_\Gamma \diff \mathbf{x}\, p_A(\mathbf{x})\ln p_A(\mathbf{x})\right)-\int_\Gamma \diff \mathbf{x}\, p_A(\mathbf{x})\ln p_B(\mathbf{x}),
\end{align}
where $H(p_A)=-\int_\Gamma \diff \mathbf{x}\, p_A(\mathbf{x})\ln p_A(\mathbf{x})$ is the entropy of the distribution under Hamiltonian $A$, and $-\int_\Gamma \diff \mathbf{x}\, p_A(\mathbf{x})\ln p_B(\mathbf{x})$ is the cross-entropy between the distribution under Hamiltonian $A$ and that under Hamiltonian $B$. $KL(p_A||p_B)$ is also known as the relative entropy in Information theory. Similarly, we can also have
\begin{equation}
	KL(p_B||p_A)=\int_\Gamma \diff \mathrm{x}\, p_B(\mathbf{x})\ln\left[\frac{p_B(\mathbf{x})}{p_A(\mathbf{x})}\right].
\end{equation}
With the Boltzmann statistics,
\begin{equation}
	p_A(\mathbf{x})=e^{-\beta U(\mathbf{x})}/Q_A.
\end{equation}
The relative entropies can be written as
\begin{align}
	KL(p_A||p_B)=&\int_\Gamma \diff \mathrm{x}\, \frac{e^{-\beta U_A(\mathbf{x})}}{Q_A}\ln\left[\frac{e^{-\beta U_A(\mathbf{x})}}{Q_A}\frac{Q_B}{e^{-\beta U_B(\mathbf{x})}}\right]\notag\\
	   =&\int_\Gamma \diff \mathrm{x}\, \frac{e^{-\beta U_A(\mathbf{x})}}{Q_A} \left\{\beta \left[U_B(\mathbf{x})-U_A(\mathbf{x})\right]-\beta \Delta A\right\}\\
	   =&\beta\left<\Delta U\right>_A-\beta \Delta A
\end{align}
and
\begin{equation}
	KL(p_B||p_A)=-\beta\left<\Delta U\right>_B+\beta \Delta A,
\end{equation}
which are exactly the dissipated work\cite{WuJCP2005}.

\section{Mutual information\label{Sec:Eva:MI}}
The definition of entropy
\begin{equation}
	H(p)=-\int \diff \mathbf{x}\, p(\mathbf{x})\ln p(\mathbf{x})
\end{equation}
can be extended to joint distributions as
\begin{equation}
	H(p)=\iint \diff \mathbf{x}\diff \mathbf{y}\, p(\mathbf{x},\mathbf{y})\ln p(\mathbf{x},\mathbf{y})
\end{equation}

The conditional entropy $H(\mathbf{Y}|\mathbf{X})$ is the amount of information needed to determine $\mathbf{Y}$ when $\mathbf{X}$ is known
\begin{align}
	H(\mathbf{Y}|\mathbf{X})=&\int \diff \mathbf{x}\, p(\mathbf{x})  H(\mathbf{Y}|\mathbf{X}=\mathbf{x})\notag\\
	  =&-\iint \diff \mathbf{x}\diff \mathbf{y}\, p(\mathbf{x},\mathbf{y})\log{p(\mathbf{y}|\mathbf{x})}.
\end{align}
The chain rule
\begin{equation}
	H(\mathbf{X}_{1:n})=H(\mathbf{X}_1)+H(\mathbf{X}_2|\mathbf{X}_1)+\dots+H(\mathbf{X}_n|\mathbf{X}_{1:n-1})
\end{equation}
can be easily derived from above. In general, $H(\mathbf{Y}|\mathbf{X})\neq H(\mathbf{X}|\mathbf{Y})$ , and $H(\mathbf{Y}|\mathbf{X})=H(\mathbf{Y})$ when $\mathbf{X}$ and $\mathbf{Y}$ are independent.

From the chain rule above, it can be seen that
\begin{equation}
	H(\mathbf{X},\mathbf{Y})=H(\mathbf{X})+H(\mathbf{Y}|\mathbf{X})=H(\mathbf{Y})+H(\mathbf{X}|\mathbf{Y}).
\end{equation}
By switching the terms, it can be found that
\begin{equation}
	H(\mathbf{X})-H(\mathbf{X}|\mathbf{Y})=H(\mathbf{Y})-H(\mathbf{Y}|\mathbf{X}),
\end{equation}
which can be interpreted as the entropy reduction in $\mathbf{X}$ when $\mathbf{Y}$ is known, or vice versa. The mutual information is thus defined
\begin{align}
	I(\mathbf{X};\mathbf{Y})=&H(\mathbf{X})-H(\mathbf{X}|\mathbf{Y})\notag\\
	=&H(\mathbf{Y})-H(\mathbf{Y}|\mathbf{X})\notag\\
	=&\iint \diff \mathbf{x}\diff \mathbf{y}\, p(\mathbf{x},\mathbf{y})\log{\frac{p(\mathbf{x},\mathbf{y})}{p(\mathbf{x})p(\mathbf{y})}}.
\end{align}
It can also be written as
\begin{equation}
	I(\mathbf{X};\mathbf{Y})=KL\left(p(\mathbf{x},\mathbf{y})||p(\mathbf{x})p(\mathbf{y})\right).
\end{equation}
