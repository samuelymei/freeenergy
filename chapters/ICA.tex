% !TeX spellcheck = en_US
\section{Independent Component Analysis\label{Sec:DR:ICA}}
TODO: to check the equations.

Independent component analysis (ICA) is a special case of blind source separation, which is used for separating a multivariate signal into additive subcomponents. ICA is considered as an extension of the principal component analysis (PCA, see section~\ref{Sec:DR:PCA}) technique. However, PCA searches uncorrelated components while ICA looks for independent components. For a not very recent review, please refer to Ref.~\cite{HyvarinenNN2000}.

ICA is built based on three assumptions. 
\begin{enumerate}
	\item \textbf{Independence}: The source signals are independent of each other.
	\item \textbf{Non-Gaussianity}: The mixed signals are Gaussian, but the values in each source signal have non-Gaussian distributions.
	\item \textbf{Complexity}: Mixed signals are more complex than source signals. 
\end{enumerate}

The \textit{signals} must be preprocessed before they can be projected to find the unmixing matrix and the \textit{sources}. The preprocessing steps include centering, whitening and dimensionality reduction. Suppose a random $r$-dimensional vector $\mathbf{X}=(X_1,\dots,X_r)^{\operatorname{T}}$ has been detected, of which the mean and the covariance matrix are $\operatorname{E}\{\mathbf{X}\}=\boldsymbol{\mu}$ and $\operatorname{cov}\{\mathbf{X}\}=\boldsymbol{\Sigma}_{\mathbf{XX}}$, respectively. These \textit{signals} should be preprocessed by first centering so that they have zero mean, and then by sphering (or whitening) so that the uncorrelated components have unit variances. Using spectral decomposition, we have $\boldsymbol{\Sigma}_{\mathbf{XX}}=\mathbf{U}\boldsymbol{\Lambda}\mathbf{U}^{\operatorname{T}}$, where the columns of the unitary matrix $\mathbf{U}$ are the eigenvectors of $\boldsymbol{\Sigma}_{\mathbf{XX}}$, and the diagonal elements of the diagonal matrix $\boldsymbol{\Lambda}$ are the corresponding eigenvalues. The centered and sphered version of $\mathbf{X}$ can be given by
\begin{equation}
    \mathbf{Z}\leftarrow \boldsymbol{\Lambda}^{-1/2}\mathbf{U}^{\operatorname{T}}(\mathbf{X}-\boldsymbol{\mu}).
\end{equation}
In the above, we have assumed that both $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}_{\mathbf{XX}}$ are known. When they are unknown as in most cases in practice, we take $n$ observations, $\mathbf{x}_1, \dots, \mathbf{x}_n$, on $\mathbf{X}$ to compute $\hat{\boldsymbol{\mu}}=n^{-1}\sum_{i=1}^n\mathbf{x}_i$ and $\hat{\boldsymbol{\Sigma}}_{\mathbf{XX}}=n^{-1}\sum_{i=1}^n(\mathbf{x}_i-\hat{\boldsymbol{\mu}})(\mathbf{x}_i-\hat{\boldsymbol{\mu}})^{\operatorname{T}}=\hat{\mathbf{U}}\hat{\boldsymbol{\Lambda}}\hat{\mathbf{U}}^{\operatorname{T}}$. To reduce the dimensionality of the data, only the first $J < r$ sphered variables (corresponding to the eigenvectors with the largest magnitudes of the eigenvalues) are retained, where $J$ is chosen to explain a certain (high) proportion of the total variance as we do in PCA.

The observed data set $\mathbf{X}=(X_1,\dots,X_r)^{\operatorname{T}}$ are generated by
\begin{equation}
	\mathbf{X}=f(\mathbf{S})+\mathbf{e},
	\label{eq:DR:ICA:signalfromsource}
\end{equation}
where $\mathbf{S}=(S_1,\dots,S_m)^{\operatorname{T}}$ is an unknown vector of source whose components are independent latent variables, $f:\mathbb{R}^m\to \mathbb{R}^r$ is an unknowing mixing function, and $\mathbf{e}$ represents measurement noise with a zero mean. We assume that $\boldsymbol{\operatorname{E}}(\mathbf{S})=\mathbf{0}$ and $\boldsymbol{\operatorname{cov}}(\mathbf{S})=\mathbf{I}_m$. 

\textit{Nomenclature}: If $f$ is taken to be a linear (nonlinear) function, Eq.~\ref{eq:DR:ICA:signalfromsource} is described as a \textit{linear} (\textit{nonlinear}) ICA model. In most applications of ICA, it is assumed that the additive noise $\mathbf{e}$ is zero, and all noise in the model is to be associated with the components of the source. Such a model is referred to as \textit{noiseless} ICA. Otherwise, it is referred to as \textit{noisy} ICA. In most ICA applications, $\mathbf{X}$ is regarded as a stochastic process $\mathbf{X}(t) = (X_1(t), \dots, X_r(t))^{\operatorname{T}}$, where $t$ is a time or index parameter. In the linear noiseless ICA model with temporally structured sources and \textit{static} (time-independent) mixing, the model is written as $\mathbf{X}(t)=\mathbf{A}\mathbf{S}(t)$, where $\mathbf{S}(t)$ is assumed to be a \textit{stationary sources}. If $\mathbf{A}$ is also time dependent, this model is referred to as \textit{dynamic mixing}. In the following, we only consider \textit{static} mixing.

With the observed data set $\mathbf{X}=(X_1,\dots,X_r)^{\operatorname{T}}$ as an input, the task of ICA is to transform $\mathbf{X}$ into a vector of source with maximally independent components $\mathbf{Y}=(Y_1,\dots,Y_m)^{\operatorname{T}}$ using a linear static transformation $\mathbf{W}$ as $\mathbf{Y}=\mathbf{WX}$. The independence is measured by some function $F(\mathbf{Y})$. ICA finds the independent components (also called factors, latent variables or sources) by maximizing the statistical independence of the estimated components. There are many ways to define a proxy for independence, and the choice governs the form of the ICA algorithm, such as by 1) minimization of mutual information, which used measures like Kullback-Leibler Divergence and maximum entropy, 2) maximization of non-Gaussianity, which uses kurtosis and negentropy, and 3) using maximum likelihood estimation method. 

\subsection{Maximizing the non-Gaussianity\label{Sec:DR:ICA:MnG}}
Non-Gaussianity can be measured by kurtosis and negative entropy.
\subsubsection{Kurtosis\label{Sec:DR:ICA:MnG:Kurtosis}}
The signal (sources) can be extracted by finding the orientation of the weight vectors which maximizes the kurtosis of the extracted signal. \textit{Although it is simple to calculate, it is sensitive to outliers. Therefore, it is not a robust way to measure the non-Gaussianity.} The kurtosis ($K$) for any probability density function is defined as
\begin{equation}
	K(\mathbf{X})=\operatorname{E}\left[\mathbf{X}^4\right]-3\left[\operatorname{E}\left[\mathbf{X}^2\right]\right]^2.
\end{equation}
The normalized kurtosis ($\hat{K}$) is the ratio between the fourth and second central moments, and it is given by
\begin{equation}
	\hat{K}=\frac{\operatorname{E}\left[\mathbf{X}^4\right]}{\left[\operatorname{E}\left[\mathbf{X}^2\right]\right]^2}-3\approx \frac{\frac{1}{N}\sum_{i=1}^N\left(X_i-\mu\right)^4}{\left(\frac{1}{N}\sum_{i=1}^N\left(X_i-\mu\right)^2\right)^2}-3
\end{equation}
For whitened data $\mathbf{Z}$ with a unit variance and zero mean,
\begin{equation}
	K(\mathbf{Z})=\hat{K}(\mathbf{Z})=\operatorname{E}\left[\mathbf{Z}^4\right]-3.
\end{equation}

The ICs can be found by maximizing kurtosis of extracted signals $\mathbf{Y}=\mathbf{W}^{\operatorname{T}}\mathbf{Z}$, which can be written as
\begin{equation}
	K(\mathbf{Y})={\operatorname{E}}\left[\left(\mathbf{W}^{\operatorname{T}}\mathbf{Z}\right)^4\right]
\end{equation}
with the gradient being
\begin{equation}
	\frac{\partial K(\mathbf{W}^{\operatorname{T}}\mathbf{Z})}{\partial \mathbf{W}}=c{\operatorname{E}}\left[\mathbf{Z}\left(\mathbf{W}^{\operatorname{T}}\mathbf{Z}\right)^3\right].
\end{equation}
The weight vector is updated iteratively via
\begin{equation}
	\mathbf{W}_{new}=\mathbf{W}_{old}+\eta {\operatorname{E}}\left[\mathbf{Z}\left(\mathbf{W}^{\operatorname{T}}\mathbf{Z}\right)^3\right],
\end{equation}
and
\begin{equation}
	\mathbf{W}_{new}=\frac{\mathbf{W}_{new}}{\lVert\mathbf{W}_{new}\rVert},
\end{equation}
since $\lVert\mathbf{W}\rVert=1$.

\subsubsection{Negative entropy\label{Sec:DR:ICA:MnG:ne}}
Negative entropy, or negentropy for short, is defined as
\begin{equation}
	J(\mathbf{Y})=H(\mathbf{Y}_{Gaussian})-H(\mathbf{Y}),
\end{equation}
where $H(\mathbf{Y}_{Gaussian})$ is the entropy of a Gaussian random variable whose covariance matrix is equal to the covariance matrix of $\mathbf{Y}$. The entropy of a random variable $\mathbf{Y}$ which has $N$ possible outcomes is
\begin{equation}
	H(\mathbf{Y})=-\operatorname{E}\left[\log{p_y(y)}\right]=-\sum_i^N p_y(y^i)\log{p_y(y^i)},
\end{equation}
where $p_y(y^i)$ is the probability of the event $y^i,\, i=1,\dots, N$. The negentropy is always nonnegative because the entropy of a Gaussian distribution is the maximum among all other random distributions with the same variance. It is zero only when all variables are Gaussian distributed, i.e., $H(\mathbf{Y}_{Gaussian})=H(\mathbf{Y})$. Moreover, it is invariant for invertible linear transformation and scale-invariant. However, calculating the entropy from a finite data is computationally difficult. Hence, different approximations have been introduced. For example,
\begin{equation}
	J(y)\approx \sum_{i=1}^p k_i\left({\operatorname{E}}\left[G_i(y)\right]-{\operatorname{E}}\left[G_i(v)\right]\right)^2,
\end{equation} 
where $k_i$ are some positive constants, $v$ indicates a Gaussian variable with zero mean and unit variance, $G_i$ represent some quadratic function. The function $G$ has different choices such as
\begin{equation}
	G_1(y)=\frac{1}{a}\log\cosh (a_1y) \quad G_2(y)=-\exp(-y^2/2),
\end{equation}
where $1\leq a_1 \leq 2$.

\subsection{Minimization of mutual information\label{Sec:DR:ICA:MMI}}
The amount of mutual information between the $m$ components of $\mathbf{Y}$ can be written as
\begin{equation}
	I(\mathbf{Y})=c-\sum_{j=1}^m J(Y_j),
\end{equation}
where $c=mH(\mathbf{Y}_{Gaussian})-H(\mathbf{X})$ does not depend on the unmixing matrix $\mathbf{W}$ and is a constant. Therefore, minimizing the mutual information between the components of $\mathbf{Y}$ is equivalent to maximizing the sum of the negentropies of the independent components of $\mathbf{Y}$.

\subsection{Maximum likelihood\label{Sec:DR:ICA:ML}}
For a noise-free ICA model, $\mathbf{X}=\mathbf{AS}$. Hence,
\begin{equation}
	P_\mathbf{X}(\mathbf{X})=\frac{P_\mathbf{S}(\mathbf{S})}{|{\operatorname{det}}\mathbf{A}|}=|{\operatorname{det}}\mathbf{W}|P_\mathbf{S}(\mathbf{S}).
\end{equation}
For independent source signals, $P_\mathbf{S}(\mathbf{S})=\prod_ip_i(\mathbf{s}_i)$, $P_\mathbf{X}(\mathbf{X})$ becomes
\begin{equation}
	P_\mathbf{X}(\mathbf{X})=|{\operatorname{det}}\mathbf{W}|\prod_ip_i(\mathbf{s}_i)=|{\operatorname{det}}\mathbf{W}|\prod_ip_i(\mathbf{w}_i^T\mathbf{X}).
\end{equation}
Given $T$ observations of $\mathbf{X}$, the likelihood of $\mathbf{W}$ is given by
\begin{equation}
	\mathcal{L}(\mathbf{W})=\prod_t^T|{\operatorname{det}}\mathbf{W}|\prod_i^p p_i(\mathbf{w}_i^T\mathbf{x}(t)).
\end{equation}
Usually, a log-likelihood is preferred:
\begin{equation}
	{\operatorname{log}}\mathcal{L}(\mathbf{W})=\sum_t^T\sum_i^p{\operatorname{log}}p_i(\mathbf{w}_i^T\mathbf{x}(t))+T\operatorname{log}|{\operatorname{det}}\mathbf{W}|,
\end{equation}
or
\begin{align}
	\frac{1}{T}{\operatorname{log}}\mathcal{L}(\mathbf{W})=&{\operatorname{E}}\left[\sum_t^T\sum_i^p{\operatorname{log}}p_i(\mathbf{w}_i^T\mathbf{x}(t))\right]+\operatorname{log}|{\operatorname{det}}\mathbf{W}|,\notag\\
	=&-\sum_i H(\mathbf{w}_i^T\mathbf{X})+\operatorname{log}|{\operatorname{det}}\mathbf{W}|.
\end{align}
Therefore, the likelihood and mutual information are approximately equal, and they only differ by the sign and an additive constant.