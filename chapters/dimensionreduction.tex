\chapter{Dimension Reduction\label{chapter:DR}}
\begin{chapquote}{Winston Churchill%, \textit{\url{https://en.wikiquote.org/wiki/Albert_Einstein}}
	}
	``Success is stumbling from failure to failure with no loss of enthusiasm.''
\end{chapquote}
Natural data are typically represented in very high dimensional spaces. It faces the problem of ``curse of dimensionality`` when analyzing these high-dimensional data. Fortunately, production of these data is often conducted in relatively few degrees of freedom. For human beings, it is difficult to interpret high dimensional data. It is usually essential to identify slowly varying order parameters, also known as collective variables (CVs), out from a complex system in physical chemistry. For processes with a huge number of time scales, it is usually accepted that the CVs should be lying on a low-dimensional manifold and capture the slow dynamics of rare events embedded in a wide of spectrum of fast events. However, it is a challenging task, and we normally resort to intuition.

For a brief introduction to the modern dimension reduction methods or manifold learning methods, please refer to Ref.~\cite{IzamanWIREsCS2012}. Specially, the generic problem addressed by dimension reduction, or manifold learning, is as follows: given a set of $k$ observations $\mathbf{x}_1,\dots,\mathbf{x}_k$ in $\mathbb{R}^D$, find a set of points $\mathbf{y}_1,\dots,\mathbf{y}_k$ in $\mathbb{R}^d$ ($d\ll D$) that serve as the best representation of $\mathbf{x}_i$. Most dimensionality reduction algorithms fit into either one of two broad categories: Matrix factorization (such as PCA) or Graph layout (such as t-SNE and UMAP).

\clearpage 
\input {chapters/PCA.tex}
\clearpage
\input {chapters/MDS.tex}
\clearpage
\input {chapters/ICA.tex}
\clearpage
\input {chapters/Isomap.tex}
\clearpage
\input {chapters/LLE.tex}
\clearpage
\input {chapters/DiffusionMap.tex}
\clearpage
\input {chapters/LaplacianEigenmaps.tex}
\clearpage
\input {chapters/t-SNE.tex}
\clearpage
\input {chapters/UMAP.tex}