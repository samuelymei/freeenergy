% !TeX spellcheck = en_US
\section{Principal Component Analysis\label{Sec:DR:PCA}}
Principal component analysis (PCA) was first developed by Hotelling in 1933,\cite{HotellingJEP1933} which is one of the dimension reduction methods that \textbf{linearly} transforms a data set consisting of a large number of interrelated variables to a new set of uncorrelated variables, the principal components (PCs), while retaining as much as possible of the variation present in the data set. The output PCs are ordered so that the first few retain most of the variation present in all of the original variables. There have been many excellent review and tutorial of this method. For a (probably) most recent one, please refer to Ref.~\cite{GreenacreNRMP2022}

%Principal component analysis is a versatile statistical method that combines information linearly from $n$ variables observed on the same objects into $r$ variables, called principal components, where $r\ll n$.

Suppose that $\mathbf{x}$ is a vector of $p$ random variables, of which the covariance matrix is $\boldsymbol{\Sigma}$. When $\boldsymbol{\Sigma}$ is unknown, it is often replaced by a sample variance $\mathbf{S}$. Let $\boldsymbol{\alpha}_k$ (for $k=1,\dots, p$ ) be the $k$th eigenvector of $\boldsymbol{\Sigma}$ corresponding to its $k$th largest eigenvalue of $\lambda_k$. The coordinate on the $k$th PC can be written as
\begin{equation}
	z_k={\boldsymbol{\alpha}_k}^{\operatorname{T}} \mathbf{x}=\sum_{j=1}^p \alpha_{kj}x_j,
\end{equation}
where $\operatorname{T}$ denotes transpose. Normally, $\boldsymbol{\alpha}_k$ is chosen to have a unit length (\textit{i.e.} ${\boldsymbol{\alpha}_k}^{\operatorname{T}}\boldsymbol{\alpha}_k=1$). Then the variance of $z_k$, $\operatorname{var}(z_k)$, equals to $\lambda_k$.

To derive the form of the PCs, first consider $\boldsymbol{\alpha}_1$, which maximizes $\operatorname{var}\left[{\boldsymbol{\alpha}_1}^{\operatorname{T}}\mathbf{x}\right]={\boldsymbol{\alpha}_1}^{\operatorname{T}}\boldsymbol{\Sigma}\boldsymbol{\alpha}_1$ subject to ${\boldsymbol{\alpha}_1}^{\operatorname{T}}\boldsymbol{\alpha}_1=1$. Using the technique of Lagrange multipliers, it becomes to maximize
\begin{equation}
	{\boldsymbol{\alpha}_1}^{\operatorname{T}}\boldsymbol{\Sigma}\boldsymbol{\alpha}_1-\lambda ({\boldsymbol{\alpha}_1}^{\operatorname{T}}\boldsymbol{\alpha}_1-1),
\end{equation}
where $\lambda$ is a Lagrange multiplier. Differentiation with respect to $\boldsymbol{\alpha}_1$ gives
\begin{equation}
	\boldsymbol{\Sigma}\boldsymbol{\alpha}_1-\lambda \boldsymbol{\alpha}_1=0.
\end{equation}
Thus, $\lambda$ is an eigenvalue of $\boldsymbol{\Sigma}$, and $\boldsymbol{\alpha}_1$ is the corresponding eigenvector. Also note that
\begin{equation}
	{\boldsymbol{\alpha}_1}^{\operatorname{T}}\boldsymbol{\Sigma} \boldsymbol{\alpha}_1={\boldsymbol{\alpha}_1}^{\operatorname{T}} \lambda \boldsymbol{\alpha}_1=\lambda {\boldsymbol{\alpha}_1}^{\operatorname{T}} \boldsymbol{\alpha}_1=\lambda.
\end{equation}
Therefore, in order to maximize ${\boldsymbol{\alpha}_1}^{\operatorname{T}}\boldsymbol{\Sigma} \boldsymbol{\alpha}_1$, $\lambda$ must be the largest eigenvalue of $\boldsymbol{\Sigma}$.

Now, let us look at the second PC, $\boldsymbol{\alpha}_2\mathbf{x}$, which maximizes ${\boldsymbol{\alpha}_2}^{\operatorname{T}}\boldsymbol{\Sigma} \boldsymbol{\alpha}_2$ subject to being uncorrelated with the first PC, $\boldsymbol{\alpha}_1\mathbf{x}$, \textit{i.e.} $\operatorname{cov}\left[{\boldsymbol{\alpha}_1}^{\operatorname{T}} \mathrm{x}, {\boldsymbol{\alpha}_2}^{\operatorname{T}} \mathrm{x}\right]=0$. Since
\begin{equation}
	\operatorname{cov}\left[{\boldsymbol{\alpha}_1}^{\operatorname{T}} \mathrm{x}, {\boldsymbol{\alpha}_2}^{\operatorname{T}} \mathrm{x}\right]={\boldsymbol{\alpha}_1}^{\operatorname{T}} \boldsymbol{\Sigma} \boldsymbol{\alpha}_2={\boldsymbol{\alpha}_2}^{\operatorname{T}} \boldsymbol{\Sigma} \boldsymbol{\alpha}_1=\lambda_1 {\boldsymbol{\alpha}_2}^{\operatorname{T}} \boldsymbol{\alpha}_1,
\end{equation}
any one of the following equations
\begin{equation}
\begin{array}{rr}
	{\boldsymbol{\alpha}_1}^{\operatorname{T}} \boldsymbol{\Sigma} \boldsymbol{\alpha}_2=0, & {\boldsymbol{\alpha}_2}^{\operatorname{T}} \boldsymbol{\Sigma} \boldsymbol{\alpha}_1=0 \\
	{\boldsymbol{\alpha}_1}^{\operatorname{T}} \boldsymbol{\alpha}_2=0, & {\boldsymbol{\alpha}_2}^{\operatorname{T}} \boldsymbol{\alpha}_1=0
\end{array}
\end{equation}
could be used to specify the constraint. Using, for instance, the last one, as well as the normalization condition, the quantity to be maximized is
\begin{equation}
	{\boldsymbol{\alpha}_2}^{\operatorname{T}} \boldsymbol{\Sigma} \boldsymbol{\alpha}_2-\lambda\left({\boldsymbol{\alpha}_2}^{\operatorname{T}} \boldsymbol{\alpha}_2-1\right)-\phi {\boldsymbol{\alpha}_2}^{\operatorname{T}} \boldsymbol{\alpha}_1,
\end{equation}
where $\lambda$ and $\phi$ are Lagrange multipliers. Differentiation with respect to $\boldsymbol{\alpha}_2$ gives
\begin{equation}
	\boldsymbol{\Sigma} \boldsymbol{\alpha}_2-\lambda \boldsymbol{\alpha}_2-\phi \boldsymbol{\alpha}_1=\mathbf{0}.
\end{equation}
Multiplying on the left by ${\boldsymbol{\alpha}_1}^{\operatorname{T}}$ gives
\begin{equation}
	{\boldsymbol{\alpha}_1}^{\operatorname{T}} \boldsymbol{\Sigma} \boldsymbol{\alpha}_2-\lambda {\boldsymbol{\alpha}_1}^{\operatorname{T}} \boldsymbol{\alpha}_2-\phi {\boldsymbol{\alpha}_1}^{\operatorname{T}} \boldsymbol{\alpha}_1=0.
\end{equation}
Since the first two terms are zero and ${\boldsymbol{\alpha}_1}^{\operatorname{T}} \boldsymbol{\alpha}_1=1$, it leads to $\phi=0$. Therefore,
\begin{equation}
	\boldsymbol{\Sigma} \boldsymbol{\alpha}_2-\lambda \boldsymbol{\alpha}_2=\mathbf{0},
\end{equation}
indicating that $\lambda$ is an eigenvalue of $\boldsymbol{\Sigma}$ again, and $\boldsymbol{\alpha}_2$ the corresponding eigenvector. We can keep on doing this analysis for the third, fourth, $\dots$, $p$th PCs, and show that $\lambda_3$, $\lambda_4$, $\dots$, $\lambda_p$ are the third, fourth largest, $\dots$, and the smallest eigenvalue of $\boldsymbol{\Sigma}$, and $\boldsymbol{\alpha}_3$, $\boldsymbol{\alpha}_4$, $\dots$, $\boldsymbol{\alpha}_p$ are the corresponding eigenvectors. Furthermore,
\begin{equation}
	\operatorname{var}\left[{\boldsymbol{\alpha}_k}^{\operatorname{T}} \mathbf{x}\right]=\lambda_k \quad \text { for } k=1,2, \ldots, p.
\end{equation}


\textbf{Kernel Principal Component Analysis} (Kernel PCA): Let us suppose all the data have being mapped into feature space, $\boldsymbol{\phi}(\mathbf{x}_1),\dots,\boldsymbol{\phi}(\mathbf{x}_l)$, and are centered, i.e. $\sum_{k=l}^l \boldsymbol{\phi}(\mathbf{x}_k)=0$. PCA for the covariance matrix
\begin{equation}
	\bar{C}=\frac{1}{l}\sum_{j=1}^l \boldsymbol{\phi}(\mathbf{x}_j)\boldsymbol{\phi}(\mathbf{x}_k)^{\operatorname{T}}
	\label{eq:DR:PCA:Cbar}
\end{equation}
gives eigenvalues $\lambda >0$ and eigenvectors $\mathbf{V}$ satisfying $\lambda \mathbf{V}=\bar{C} \mathbf{V}$. $\mathbf{V}$ lies in the span of $\boldsymbol{\phi}(\mathbf{x}_1),\dots,\boldsymbol{\phi}(\mathbf{x}_l)$. Therefore, the eigen equation can be written as
\begin{equation}
	\lambda (\boldsymbol{\phi}(\mathbf{x}_k)\cdot \mathbf{V})=(\boldsymbol{\phi}(\mathbf{x}_k)\cdot \bar{C}\mathbf{V}) \text{ for all }k=1,\dots,l,
	\label{eq:DR:PCA:eigenequationprime}
\end{equation}
and there exist coefficients $\alpha_1,\dots,\alpha_l$ such that
\begin{equation}
	\mathbf{V}=\sum_{i=1}^l \alpha_i \boldsymbol{\phi}(\mathbf{x}_i).
	\label{eq:DR:PCA:Vdecompose}
\end{equation}
Now let us define
\begin{equation}
	K_{ij} :=(\boldsymbol{\phi}(\mathbf{x}_i)\cdot \boldsymbol{\phi}(\mathbf{x}_j)),
\end{equation}
and take Eq.~\ref{eq:DR:PCA:Cbar} and \ref{eq:DR:PCA:Vdecompose} into Eq.~\ref{eq:DR:PCA:eigenequationprime}, it leads to
\begin{equation}
	l\lambda K\boldsymbol{\alpha} =K^2\boldsymbol{\alpha},
\end{equation}
where $\boldsymbol{\alpha}$ is the column vector with elements $\alpha_1,\dots,\alpha_l$. Its solutions can be found by solving the eigenvalue problem
\begin{equation}
	l\lambda \boldsymbol{\alpha} =K\boldsymbol{\alpha}
\end{equation}
for nonzero eigenvalues. Enforcing the eigenvectors to be normalized, i.e. $(\mathbf{V}^k\cdot \mathbf{V}^k)=1$, the solution $\boldsymbol{\alpha}^k$ must satisfy
\begin{equation}
	1=\sum_{i,j=1}^l \alpha_i^k\alpha_j^k(\boldsymbol{\phi}(\mathbf{x}_i)\cdot \boldsymbol{\phi}(\mathbf{x}_j))=(\boldsymbol{\alpha}^k\cdot K\boldsymbol{\alpha}^k)=\lambda_k(\boldsymbol{\alpha}^k\cdot \boldsymbol{\alpha}^k).
\end{equation}
Projections of the data in the feature space $\boldsymbol{\phi}(\mathbf{x})$ onto the eigenvectors $\mathbf{V}^k$ can be computed via
\begin{equation}
	(\mathbf{V}^k\cdot \boldsymbol{\phi}(\mathbf{x}))=\sum_{i=1}^l\alpha_i^k(\boldsymbol{\phi}(\mathbf{x})\cdot \boldsymbol{\phi}(\mathbf{x})),
\end{equation}
which gives the principal components.


 