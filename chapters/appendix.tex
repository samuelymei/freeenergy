%\setcounter{section}{0}
%%\setcounter{theorem}{0}
%%\renewcommand{\thetheorem}{\thechapter.\arabic{theorem}}
%\setcounter{equation}{0}
%\setcounter{figure}{0}
%\setcounter{table}{0}
%\appendix
%\addtocontents{toc}{\cftpagenumbersoff{chapter}} 
%\chapter{Appendix}
%\addtocontents{toc}{\protect\contentsline {chapter}{Appendix}{}{}}

%\addtocontents{toc}{\cftpagenumberson{chapter}} 

%\makeatletter
%\addtocontents{toc}{\let\protect\l@chapter\protect\l@section}
%\makeatother
%\renewcommand{\thechapter}{A} 
%\renewcommand{\theequation}{\thechapter.\arabic{equation}}
%\renewcommand{\thefigure}{\thechapter.\arabic{figure}}
\begin{appendices}
%	\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
%	\makeatletter
%	\addtocontents{toc}{%
%		\begingroup
%		\let\protect\l@chapter\protect\l@section
%		\let\protect\l@section\protect\l@subsection
%	}
%	\makeatother

\chapter{Statistical Uncertainty in the Estimator for Correlated Time Series Data\label{chapter:Appendix:Uncertainty}}
\begin{chapquote}{Nassim Nicholas Taleb%, \textit{\url{https://en.wikiquote.org/wiki/Albert_Einstein}}
	}
	``It is not the estimate or the forecast that matters so much as the degree of confidence with the opinion.''
\end{chapquote}
Suppose we have a time series of correlated sequential observations of the randomly sampled variable $X$ denoted as $\{x_n\}, n=1,\dots,N$ that come from a stationary, time-reversible stochastic process. The expectation of $X$ can be estimated as the time average of the samples
\begin{equation}
  \hat X=\frac 1 N \sum\limits_{n=1}^{N} x_n.
  \label{Eq:Appendix:expectationX}
\end{equation}
Because of the existence of correlation among the samples, the variance for the expectation, which is defined as
\begin{equation}
	\delta^2 \hat X \equiv \left<\left(\hat X-\left<\hat X\right>\right)^2\right> = \left<\hat X^2 \right> -\left<\hat X\right>^2,
	\label{Eq:Appendix:uncertainty}
\end{equation}
is complicated. We first take Eq.~\ref{Eq:Appendix:expectationX} into Eq.~\ref{Eq:Appendix:uncertainty}, and split the sum into one term capturing the variance in the observations and a remaining term capturing the correlation between the observations as
\begin{align}
	\delta^2 \hat X=&\frac{1}{N^2}\sum\limits_{n,n^\prime=1}^{N}\left[\left<x_nx_{n^\prime}\right>-\left<x_n\right>\left<x_{n^\prime}\right>\right]\notag\\
	               =&\frac{1}{N^2}\sum\limits_{n=1}^{N}\left[\left<x_n^2\right>-\left<x_n\right>^2\right]+\frac{1}{N^2}\sum\limits_{n\neq n^\prime=1}^{N}\left[\left<x_nx_{n^\prime}\right>-\left<x_n\right>\left<x_{n^\prime}\right>\right]
\end{align}
Because of the stationarity, it becomes
\begin{align}
	\delta^2 \hat X=& \frac 1N\left[\left<x_n^2\right>-\left<x_n\right>^2\right] \notag\\
	&+\frac 1{N^2}\sum\limits_{t=1}^{N-1}\left(N-t\right)\left[\left<x_nx_{n+t}\right>+\left<x_{n+t}x_n\right>-\left<x_n\right>\left<x_{n+t}\right>-\left<x_{n+t}\right>\left<x_n\right>\right]
\end{align}
and because of the time-reversibility, it can be further simplified to
\begin{align}
	\delta^2 \hat X=& \frac 1N\left[\left<x_n^2\right>-\left<x_n\right>^2\right] \notag\\
	&+\frac 2N\sum\limits_{t=1}^{N-1}\left(\frac{N-t}{N}\right)\left[\left<x_nx_{n+t}\right>-\left<x_n\right>\left<x_{n+t}\right>\right]\notag\\
	\equiv &\frac{{\sigma_x}^2}{N}\left(1+2\tau\right)=\frac{{\sigma_x}^2}{N/g},
\end{align}
where ${\sigma_x}^2$, statistical inefficiency $g$, and autocorrelation time $\tau$ (in units of the sampling interval) are given by
\begin{align}
	&{\sigma_x}^2 \equiv \left<x_n^2\right>-\left<x_n\right>^2\\
	&\tau \equiv \sum\limits_{t=1}^{N-1}\left(\frac{N-t}{N}\right)C_t\\
	&C_t=\frac{\left<x_nx_{n+t}\right>-\left<x_n\right>\left<x_n\right>}{{\sigma_x}^2}\\
	&g\equiv 1+2\tau
\end{align}
The quantity $g\equiv 1+2\tau>1$ can be regarded as a statistical inefficiency, in that $N/g$ gives the effective number of \textit{uncorrelated} configurations contained in the time series.

\chapter{The Optimal Mean of Independent Measurements with Uncertainties\label{chapter:Appendix:Mean}}
Suppose we have N measurements of a quantity x, which are denoted as \{$x_i$\}, with i = 1, \dots, N. Each measurement has a variance $\delta^2 x_i$. To find the optimal mean of this data set, we first write the mean of \{$x_i$\} as a weighted average of them
\begin{equation}
\bar{x}=\sum\limits_{i=1}^N a_i x_i,
\label{Eq:Appendix:Mean:mean}
\end{equation}
in which $a_i$ are the normalized weights, i.e.
\begin{equation}
\sum\limits_{i=1}^Na_i=1.
\label{Eq:Appendix:Mean:normalization}
\end{equation} 
According to the error propagation rule, if the measurements are independent, the variance of the mean $\bar{x}$ can be written as
\begin{equation}
\delta^2\bar{x}=\sum\limits_{i=1}^N{a_i}^2\delta^2x_i.
\label{Eq:Appendix:Mean:variaceofmean}
\end{equation}
Minimizing $\delta^2\bar{x}$ with respect to $a_i$ under the constraint of Eq.~\ref{Eq:Appendix:Mean:normalization} using the Lagrange multiplier $\lambda$, we find
\begin{align}
\frac{\partial L}{\partial a_j}=&\frac{\partial}{\partial a_j}\left[\sum\limits_{i=1}^{N}{a_i}^2\delta^2x_i+\lambda\left(1-\sum\limits_{i=1}^Na_i\right)\right]\notag\\
=&2a_j\delta^2x_j-\lambda\notag\\
=&0
\end{align}
for all {$x_j$}. It can be easily identified that $a_j$ is inversely proportional to $\delta^2x_j$, i.e.
\begin{equation}
a_j=\frac{{\left(\delta^2x_j\right)}^{-1}}{\sum\limits_{i=1}^N {\left(\delta^2x_i\right)}^{-1}},
\label{Eq:Appendix:Mean:aj}
\end{equation}
and
\begin{equation}
\bar{x}=\sum\limits_{j=1}^N \frac{{\left(\delta^2x_j\right)}^{-1}}{\sum\limits_{i=1}^N {\left(\delta^2x_i\right)}^{-1}} x_j,
\label{Eq:Appendix:Mean:meanexpanded}
\end{equation}
with
\begin{equation}
\delta^2\bar{x}=\sum\limits_{j=1}^N \frac{{\left(\delta^2x_j\right)}^{-1}}{\left(\sum\limits_{i=1}^N {\left(\delta^2x_i\right)}^{-1}\right)^2}=\frac{1}{\sum\limits_{i=1}^N {\left(\delta^2x_i\right)}^{-1}}.
\label{Eq:Appendix:Mean:varofmeanexpanded}
\end{equation}

\chapter{Fluctuation Theorem\label{chapter:Appendix:FT}}
The fluctuation theorem is a fundamental result in non-equilibrium statistical mechanics that quantifies the probability distribution of the entropy production in a system far from equilibrium.\cite{EvansPRL1993} It provides a rigorous mathematical framework for understanding the statistical properties of systems that are driven out of equilibrium by external forces. The fluctuation theorem has been experimentally verified in various systems, including colloidal particles, single-molecule experiments, and electronic systems.

In classical thermodynamics, the second law states that the entropy of an isolated system never decreases. This macroscopic law does not provide detailed information about the microscopic fluctuations that occur in small systems or over short timescales. The fluctuation theorem addresses this limitation by describing the statistical distribution of entropy production and its fluctuations.

\textbf{Statement of the Fluctuation Theorem}

Consider a system in contact with a thermal reservoir at temperature $T$. Let $\Sigma(t)$ denote the entropy production over a time interval $t$. The fluctuation theorem states that the probability distribution of the entropy production satisfies the following relation:

\begin{equation}
	\frac{P(\Sigma(t) = A)}{P(\Sigma(t) = -A)} = e^{A/k_B},
\end{equation}
where $P(\Sigma(t) = A)$ is the probability that the entropy production is $A$ over the time interval $t$, and $k_B$ is the Boltzmann constant.

\textbf{Derivation of the Fluctuation Theorem}

The fluctuation theorem can be derived using the principles of statistical mechanics and the concept of time-reversal symmetry. Here, we outline the key steps of the derivation:

1. Microstates and Entropy Production: Consider the set of microstates $\Gamma$ of the system. Let $\rho(\Gamma, t)$ be the probability density of finding the system in microstate $\Gamma$ at time $t$. The entropy production $\Sigma(t)$ can be related to the probability densities of the forward and time-reversed trajectories.

2. Time-Reversal Symmetry: Under time reversal, the system evolves along a trajectory $\Gamma'$. The probability density $\rho(\Gamma, t)$ transforms into $\rho(\Gamma', t)$, and the entropy production changes sign.

3. Ratio of Probabilities: The ratio of the probability densities for forward and time-reversed trajectories is given by the exponential of the entropy production:
\begin{equation}
	\frac{\rho(\Gamma, t)}{\rho(\Gamma', t)} = e^{\Sigma(t)/k_B}.
\end{equation}

4. Ensemble Averaging: Averaging over all possible microstates and trajectories, we obtain the fluctuation theorem:
\begin{equation}
	\frac{P(\Sigma(t) = A)}{P(\Sigma(t) = -A)} = e^{A/k_B}.
\end{equation}

\textbf{Implications and Applications}

The fluctuation theorem has several important implications:

1. Violation of the Second Law: For small systems or over short timescales, there is a non-zero probability of observing entropy decreases, which appears to violate the second law of thermodynamics. However, these fluctuations are exponentially suppressed for large systems or long timescales.

2. Non-Equilibrium Work Relations: The fluctuation theorem is closely related to work relations such as the Jarzynski equality\cite{JarzynskiPRL1997} and Crooks fluctuation theorem\cite{CrooksPRE1999}, which provide insights into the work done on or by a system during non-equilibrium processes.

\chapter{The Relationship between the $\Delta U$ Distributions in Forward and Backward TP\label{chapter:Appendix:DeltaUDistributions}}
In a forward TP calculation between $H_0$ and $H_1$, the distribution of $\Delta U$ is
\begin{equation}
    f(\Delta U)=\frac{{\displaystyle\int} e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}{{\displaystyle\int} e^{-\beta H_0}\diff x}.
\end{equation}
While in a backward TP, the distribution is
\begin{align}
    g(\Delta U)=&\frac{\displaystyle\int e^{-\beta H_1}\delta(H_1-H_0-\Delta U)\diff x}{\displaystyle\int e^{-\beta H_1}\diff x}\notag\\
               =&\frac{f(\Delta U)\displaystyle\int e^{-\beta H_1}\delta(H_1-H_0-\Delta U)\diff x\int e^{-\beta H_0}\diff x}{\displaystyle\int e^{-\beta H_1}\diff x\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}\notag\\
               =&f(\Delta U)\frac{\displaystyle\int e^{-\beta H_0}\diff x }{\displaystyle\int e^{-\beta H_1}\diff x}\frac{\displaystyle\int e^{-\beta H_1}\delta(H_1-H_0-\Delta U)\diff x}{\displaystyle\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}\notag\\
               =&f(\Delta U)e^{\beta\Delta A}\frac{\displaystyle\int e^{-\beta\Delta H}e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}{\displaystyle\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}\notag\\
               =&f(\Delta U)e^{\beta\Delta A}\frac{e^{-\beta\Delta U}\displaystyle\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}{\displaystyle\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}\notag\\
               =&f(\Delta U)e^{\beta\Delta A}e^{-\beta\Delta U}
\end{align}
Or it can be written as
\begin{equation}
    g(\Delta U)e^{\beta\Delta U}=f(\Delta U)e^{\beta\Delta A},
\end{equation}
which was first shown by Shing and Gubbins for Widom insertion and deletion\cite{ShingMolPhys1982}.

By looking at this equation and the integral in energy space for TP (Eq.~\ref{Eq:FEM:TP:deltaA7}), 
\begin{equation}
	\Delta A = -\frac{1}{\beta} \ln \int \exp(-\beta \Delta U) f(\Delta U) \diff\Delta U,
\end{equation}
it can be easily realized that the integrand is proportional to the probability of energy difference sampled from state 1 ($g(\Delta U)$) and the free energy difference $\Delta A$ can be estimated reliably only when the sampling at state $0$ covers the representative configurations of state $1$.

\chapter{Cumulant Expansion for the Free Energy Difference in Thermodynamic Perturbation Calculations\label{chapter:Appendix:CE}}
In Thermodynamic Perturbation, the free energy difference between states are expressed as
\begin{equation}
	\Delta A=-\beta^{-1}\ln{\left<\exp{\left[-\beta \Delta U\right]}\right>}_0.
	\label{Eq:Appendix:CE:Exp}
\end{equation}
Due to the exponential term on the right-hand-side of this equation, the convergence is usually slow. This expression can be expanded into Taylor series, and the leading terms can converge much faster than the complete sum.

Using the Taylor expansion for $e^x=1+\sum\limits_{n=1}^{\infty}\frac{1}{n!}x^n$ first, we have
\begin{equation}
	\exp{\left[-\beta \Delta U\right]}=1+(-\beta \Delta U)+\frac{1}{2!}(-\beta \delta U)^2+\frac{1}{3!}(-\beta \Delta U)^3+\cdots.
\end{equation}
Then Eq.~\ref{Eq:Appendix:CE:Exp} becomes
\begin{equation}
	\Delta A=-\beta^{-1}\ln{\left(1-\beta\left<\Delta U\right>_0+\frac{1}{2}\beta^2\left<\Delta U^2\right>_0-\frac{1}{6}\beta^3\left<\Delta U^3\right>_0+\cdots\right)}.
\end{equation}
Using the Taylor expansion for $\ln{(1+x)}=x-\frac{1}{2}x^2+\frac{2}{3!}x^3-\frac{6}{4!}x^4+\cdots$, we have
\begin{align}
	\Delta A=&\left<\Delta U\right>_0-\frac{1}{2}\beta\left<\Delta U^2\right>_0+\frac{1}{6}\beta^2\left<\Delta U^3\right>_0+\cdots\notag\\
	         &+\frac{1}{2}\left(\beta\left<\Delta U\right>_0^2-\beta^2\left<\Delta U\right>_0\left<\Delta U^2\right>_0+\frac{1}{4}\beta^3\left<\Delta U^2\right>_0^2+\cdots\right)\notag\\
	         &-\frac{1}{3}\left(-\beta^2\left<\Delta U\right>_0^3+\cdots\right)\notag\\
	         &+\cdots\notag\\
	        =&\left<\Delta U\right>_0-\frac{\beta}{2}\left(\left<\Delta U^2\right>_0-\left<\Delta U\right>_0^2\right)\notag\\
	         &+\frac{\beta^2}{6}\left(\left<\Delta U^3\right>_0-3\left<\Delta U^2\right>_0\left<\Delta U\right>_0+2\left<\Delta U\right>_0^3\right)+\cdots
\end{align}


\chapter{MBAR Returns to BAR When Only Two States Are Considered\label{chapter:Appendix:twostatMBAR2BAR}}
When there are only two states, the free energy in Eq.~\ref{Eq:FEM:MBAR:f_i_final} for the 1st state in MBAR becomes
\begin{align}
f_1=&-{\beta_1}^{-1}\ln{\sum\limits_{n=1}^N \frac{\exp{\left(-\beta_1 U_1(\mathbf{R}_n)\right)}}{\sum_{k=1}^{2}N_k\exp{\left(\beta_kf_k-\beta_kU_k(\mathbf{R}_n)\right)}}}\notag\\
   =&-{\beta_1}^{-1}\ln{\sum\limits_{j=1}^{2}\sum\limits_{n=1}^{N_j} \frac{\exp{\left(-\beta_1 U_1(\mathbf{R}_{jn})\right)}}{\sum_{k=1}^{2}N_k\exp{\left(\beta_kf_k-\beta_kU_k(\mathbf{R}_{jn})\right)}}},
\end{align}
or equivalently we have
\begin{equation}
1=\sum\limits_{n=1}^{N} \frac{\exp{\left(\beta_1 f_1-\beta_1 U_1(\mathbf{R}_{n})\right)}}{N_1\exp{\left(\beta_1f_1-\beta_1U_1(\mathbf{R}_{n})\right)}+N_2\exp{\left(\beta_2f_2-\beta_2U_2(\mathbf{R}_{n})\right)}},
\end{equation}
\begin{align}
N_1=&\sum\limits_{n=1}^{N_1} \frac{1}{1+\frac{N_2}{N_1}\exp{\left(\Delta f-\Delta U(\mathbf{R}_{1n})\right)}}\notag\\
    &+\sum\limits_{n=1}^{N_2} \frac{1}{1+\frac{N_2}{N_1}\exp{\left(\Delta f-\Delta U(\mathbf{R}_{2n})\right)}}\notag\\
\end{align}
where $\Delta f=\beta_2 f_2-\beta_1 f_1$ and $\Delta U=\beta_2 U_2-\beta_1 U_1$. We further define $M=-\ln{\frac{N_2}{N_1}}$, then
\begin{align}    
N_1=&\sum\limits_{n=1}^{N_1} \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{1n})-M\right)}}\notag\\
    &+\sum\limits_{n=1}^{N_2} \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)}}\notag\\
0=&\sum\limits_{n=1}^{N_1}\left[ \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{1n})-M\right)}}-1\right]\notag\\
  &+\sum\limits_{n=1}^{N_2} \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)}}\notag
\end{align}
\begin{equation}
\sum\limits_{n=1}^{N_1}\frac{1}{1+\exp{\left(-\Delta f+\Delta U(\mathbf{R}_{1n})+M\right)}}
=\sum\limits_{n=1}^{N_2} \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)}}\notag
\end{equation}
\begin{equation}
\sum\limits_{n=1}^{N_1}f\left(-\Delta f+\Delta U(\mathbf{R}_{1n})+M\right)=\sum\limits_{n=1}^{N_2} f\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)\notag
\end{equation}
\begin{equation}
N_1\left<f\left(-\Delta f+\Delta U(\mathbf{R}_{1n})+M\right)\right>_1=N_2\left<f\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)\right>_2\notag
\end{equation}
\begin{equation}
\frac{\left<f\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)\right>_2}{\left<f\left(-\Delta f+\Delta U(\mathbf{R}_{1n})+M\right)\right>_1}=\frac{N_1}{N_2},\notag
\end{equation}
which is Eq.~\ref{Eq:FEM:BAR:BAR}.

\chapter{MBAR is a binless form of WHAM\label{chapter:Appendix:MBARandWHAM}}
Maybe you have already noticed that MBAR and WHAM have very similar forms for the free energy. 
So you may want to ask if there is any connection between MBAR and WHAM. The answer is YES. 
MBAR is a binless form of WHAM.\cite{TanJCP2012} Let us follow Zhang et al\cite{ZhangMS2016} 
and rewrite Eq.~\ref{Eq:FEM:WHAM:f_k_iteration} into an integral form
\begin{equation}
f_i=-\ln\int\Omega\exp{(-\beta_iU)}\diff U.
\label{Eq:Appendix:MBARandWHAM:f_integral}
\end{equation}
Taking Eq.~\ref{Eq:FEM:WHAM:Omega_iteration} into Eq.~\ref{Eq:Appendix:MBARandWHAM:f_integral}, we find
\begin{equation}
f_i=-\ln\int\frac{\sum\limits_{k=1}^{K}H_k(U)\exp{(-\beta_iU)}}{\sum\limits_{k=1}^{K}N_k\exp{(f_k-\beta_kU)}}\diff U,
\label{Eq:Appendix:MBARandWHAM:f_integral_whole}
\end{equation}
where ${g_{mk}}^{-1}$ has been omitted and $H_{mk}$ has been changed to continuous form $H_k(U)$. From the definition,
\begin{equation}
H_k(U)=\sum\limits_{\mathbf{R}}^{(k)}\delta (U(\mathbf{R})-U).
\label{Eq:Appendix:MBARandWHAM:H_k}
\end{equation}
Taking Eq.~\ref{Eq:Appendix:MBARandWHAM:H_k} into Eq.~\ref{Eq:Appendix:MBARandWHAM:f_integral_whole}, we have
\begin{align}
f_i=&-\ln\sum\limits_{k=1}^{K}\sum\limits_{\mathbf{R}}^{(k)}\frac{\exp{(-\beta_iU(\mathbf{R}))}}{\sum\limits_{k=1}^{K}N_k\exp{(f_k-\beta_kU(\mathbf{R}))}}\notag\\
   =&-\ln\sum\limits_{n=1}^{N}\frac{\exp{(-\beta_iU_i(\mathbf{R_n}))}}{\sum\limits_{k=1}^{K}N_k\exp{(f_k-\beta_kU_k(\mathbf{R_n}))}},
\end{align}
which is Eq.~\ref{Eq:FEM:MBAR:f_i_final}.

\chapter{Jensen's inequality\label{chapter:Appendix:JI}}
if $X$ is a random variable and $ \varphi$ is a convex function (\textit{Note: The definitions of concavity and convexity of a function by some organizations in mainland China's mathematical community are the opposite of those in foreign countries.}), then
\begin{equation}
     \varphi(\mathrm{E}[X]) \leq \mathrm{E}[\varphi(X)] .
\end{equation}

Examples:
\begin{equation}
	e^{\left<X\right>}\leq \left<e^X\right>
\end{equation}
and
\begin{equation}
	\log {\left<X\right>}\geq \log \left<X\right>
\end{equation}


\chapter{Kullback-Leibler divergence\label{chapter:Appendix:KLD}}
Let $f, g: \mathcal{X} \rightarrow[0, \infty)$ be two probability density functions on $\mathcal{X}$. The Kullback-Leibler divergence between $f$ and $g$ is defined as
\begin{equation}
     K L(f||g)=\int_{\mathcal{X}} f(x)  \log \frac{f(x)}{g(x)} \mathrm{d} x
\end{equation}
given
\begin{equation}
	\int_{\left\{g(x)=0\right\}} f(x)\diff x=0
\end{equation}

By Jensen's inequality,
\begin{align}
	K L(f||g)=\int_{\mathcal{X}} f(x)  \log \frac{f(x)}{g(x)} \mathrm{d} x=&-\int_{\mathcal{X}} f(x)  \log \frac{g(x)}{f(x)} \mathrm{d} x \notag\\
	                                                                                                               \geq& -\log \int_{\mathcal{X}} f(x)  \frac{g(x)}{f(x)} \mathrm{d} x \notag\\
	                                                                                                               =& -\log \int_{\mathcal{X}} g(x) \mathrm{d} x \notag\\
	                                                                                                               =& 0
\end{align}

\chapter{Two-sided Bogoliubov inequality\label{chapter:Appendix:BI}}
Let $\pi$ and $\pi_0$ be two probability densities,
\begin{equation}
	\pi=\frac{\exp(-H)}{Z},\quad  \pi_0=\frac{\exp(-H_0)}{Z_0}
\end{equation}
on $\mathcal{X}$. The non-negativity of the KL divergence implies that
\begin{equation}
	0\leq KL(\pi||\pi_0)=-\log\frac{Z}{Z_0}-\mathrm{E}_{\pi} [U],
\end{equation}
where $U=H-H_0$. In other words,
\begin{equation}
	\Delta F\geq \mathrm{E}_{\pi} [U].
\end{equation}
By interchanging $pi$ and $\pi_0$ in the K-L divergence, it yields
\begin{equation}
	0\leq KL(\pi_0||\pi)=\log\frac{Z}{Z_0}+\mathrm{E}_{\pi_0} [U],
\end{equation}
which leads to
\begin{equation}
	\Delta F\leq \mathrm{E}_{\pi_0} [U].
\end{equation}

Combination of these two inequalities, we arrive at the two-sided Bogoliubov inequality
\begin{equation}
	\mathrm{E}_{\pi} [U] \leq \Delta F\leq \mathrm{E}_{\pi_0} [U].
\end{equation}

\chapter{Bias-variance decomposition\label{chapter:Appendix:BVD}}
Given a set of samples, the mean squared error (M.S.E.) of an estimator $\hat{\theta}$ for some parameter $\theta$ is
\begin{equation}
	\text{M.S.E.} = \mathbb{E}{\left[\left(\hat{\theta}-\theta\right)^2\right]}.
\end{equation}
It can be decomposed into a bias term and a variance term as following
\begin{align}
	\text{M.S.E.} =& \mathbb{E}{\left[\left(\hat{\theta}-\theta\right)^2\right]}\notag\\
	       =& \mathbb{E}{\left[\left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}+\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)^2\right]}\notag\\
	       =& \mathbb{E}{\left[ \left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}\right)^2+ 2\left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}\right)\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)  +\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)^2\right]}\notag\\
	       =&\mathbb{E}{\left[ \left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}\right)^2\right]}+\mathbb{E}{\left[\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)^2\right]}\notag\\
	       =& \operatorname{Var}+\operatorname{Bias}^2,
\end{align}
where we have utilized 
\begin{equation}
	\mathbb{E}{\left[\left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}\right)\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)\right]}=\left(\mathbb{E}\left(\hat{\theta}\right)-\mathbb{E}{\left(\hat{\theta}\right)}\right)\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)=0,
\end{equation}
since $\mathbb{E}(\hat{\theta})$ and $\theta$ are constants.

%\addtocontents{toc}{\endgroup}
\end{appendices}