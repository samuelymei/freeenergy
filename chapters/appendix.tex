%\setcounter{section}{0}
%%\setcounter{theorem}{0}
%%\renewcommand{\thetheorem}{\thechapter.\arabic{theorem}}
%\setcounter{equation}{0}
%\setcounter{figure}{0}
%\setcounter{table}{0}
%\appendix
%\addtocontents{toc}{\cftpagenumbersoff{chapter}} 
%\chapter{Appendix}
%\addtocontents{toc}{\protect\contentsline {chapter}{Appendix}{}{}}

%\addtocontents{toc}{\cftpagenumberson{chapter}} 

%\makeatletter
%\addtocontents{toc}{\let\protect\l@chapter\protect\l@section}
%\makeatother
%\renewcommand{\thechapter}{A} 
%\renewcommand{\theequation}{\thechapter.\arabic{equation}}
%\renewcommand{\thefigure}{\thechapter.\arabic{figure}}
\begin{appendices}
%	\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
%	\makeatletter
%	\addtocontents{toc}{%
%		\begingroup
%		\let\protect\l@chapter\protect\l@section
%		\let\protect\l@section\protect\l@subsection
%	}
%	\makeatother

\chapter{Statistical Uncertainty in the Estimator for Correlated Time Series Data\label{chapter:Appendix:Uncertainty}}
\begin{chapquote}{Nassim Nicholas Taleb%, \textit{\url{https://en.wikiquote.org/wiki/Albert_Einstein}}
	}
	``It is not the estimate or the forecast that matters so much as the degree of confidence with the opinion.''
\end{chapquote}
Suppose we have a time series of correlated sequential observations of the randomly sampled variable $X$ denoted as $\{x_n\}, n=1,\dots,N$ that come from a stationary, time-reversible stochastic process. The expectation of $X$ can be estimated as the time average of the samples
\begin{equation}
  \hat X=\frac 1 N \sum\limits_{n=1}^{N} x_n.
  \label{Eq:Appendix:expectationX}
\end{equation}
Because of the existence of correlation among the samples, the variance for the expectation, which is defined as
\begin{equation}
	\delta^2 \hat X \equiv \left<\left(\hat X-\left<\hat X\right>\right)^2\right> = \left<\hat X^2 \right> -\left<\hat X\right>^2,
	\label{Eq:Appendix:uncertainty}
\end{equation}
is complicated. We first take Eq.~\ref{Eq:Appendix:expectationX} into Eq.~\ref{Eq:Appendix:uncertainty}, and split the sum into one term capturing the variance in the observations and a remaining term capturing the correlation between the observations as
\begin{align}
	\delta^2 \hat X=&\frac{1}{N^2}\sum\limits_{n,n^\prime=1}^{N}\left[\left<x_nx_{n^\prime}\right>-\left<x_n\right>\left<x_{n^\prime}\right>\right]\notag\\
	               =&\frac{1}{N^2}\sum\limits_{n=1}^{N}\left[\left<x_n^2\right>-\left<x_n\right>^2\right]+\frac{1}{N^2}\sum\limits_{n\neq n^\prime=1}^{N}\left[\left<x_nx_{n^\prime}\right>-\left<x_n\right>\left<x_{n^\prime}\right>\right]
\end{align}
Because of the stationarity, it becomes
\begin{align}
	\delta^2 \hat X=& \frac 1N\left[\left<x_n^2\right>-\left<x_n\right>^2\right] \notag\\
	&+\frac 1{N^2}\sum\limits_{t=1}^{N-1}\left(N-t\right)\left[\left<x_nx_{n+t}\right>+\left<x_{n+t}x_n\right>-\left<x_n\right>\left<x_{n+t}\right>-\left<x_{n+t}\right>\left<x_n\right>\right]
\end{align}
and because of the time-reversibility, it can be further simplified to
\begin{align}
	\delta^2 \hat X=& \frac 1N\left[\left<x_n^2\right>-\left<x_n\right>^2\right] \notag\\
	&+\frac 2N\sum\limits_{t=1}^{N-1}\left(\frac{N-t}{N}\right)\left[\left<x_nx_{n+t}\right>-\left<x_n\right>\left<x_{n+t}\right>\right]\notag\\
	\equiv &\frac{{\sigma_x}^2}{N}\left(1+2\tau\right)=\frac{{\sigma_x}^2}{N/g},
\end{align}
where ${\sigma_x}^2$, statistical inefficiency $g$, and autocorrelation time $\tau$ (in units of the sampling interval) are given by
\begin{align}
	&{\sigma_x}^2 \equiv \left<x_n^2\right>-\left<x_n\right>^2\\
	&\tau \equiv \sum\limits_{t=1}^{N-1}\left(\frac{N-t}{N}\right)C_t\\
	&C_t=\frac{\left<x_nx_{n+t}\right>-\left<x_n\right>\left<x_n\right>}{{\sigma_x}^2}\\
	&g\equiv 1+2\tau
\end{align}
The quantity $g\equiv 1+2\tau>1$ can be regarded as a statistical inefficiency, in that $N/g$ gives the effective number of \textit{uncorrelated} configurations contained in the time series.

\chapter{The Optimal Mean of Independent Measurements with Uncertainties\label{chapter:Appendix:Mean}}
Suppose we have N measurements of a quantity x, which are denoted as \{$x_i$\}, with i = 1, \dots, N. Each measurement has a variance $\delta^2 x_i$. To find the optimal mean of this data set, we first write the mean of \{$x_i$\} as a weighted average of them
\begin{equation}
\bar{x}=\sum\limits_{i=1}^N a_i x_i,
\label{Eq:Appendix:Mean:mean}
\end{equation}
in which $a_i$ are the normalized weights, i.e.
\begin{equation}
\sum\limits_{i=1}^Na_i=1.
\label{Eq:Appendix:Mean:normalization}
\end{equation} 
According to the error propagation rule, if the measurements are independent, the variance of the mean $\bar{x}$ can be written as
\begin{equation}
\delta^2\bar{x}=\sum\limits_{i=1}^N{a_i}^2\delta^2x_i.
\label{Eq:Appendix:Mean:variaceofmean}
\end{equation}
Minimizing $\delta^2\bar{x}$ with respect to $a_i$ under the constraint of Eq.~\ref{Eq:Appendix:Mean:normalization} using the Lagrange multiplier $\lambda$, we find
\begin{align}
\frac{\partial L}{\partial a_j}=&\frac{\partial}{\partial a_j}\left[\sum\limits_{i=1}^{N}{a_i}^2\delta^2x_i+\lambda\left(1-\sum\limits_{i=1}^Na_i\right)\right]\notag\\
=&2a_j\delta^2x_j-\lambda\notag\\
=&0
\end{align}
for all {$x_j$}. It can be easily identified that $a_j$ is inversely proportional to $\delta^2x_j$, i.e.
\begin{equation}
a_j=\frac{{\left(\delta^2x_j\right)}^{-1}}{\sum\limits_{i=1}^N {\left(\delta^2x_i\right)}^{-1}},
\label{Eq:Appendix:Mean:aj}
\end{equation}
and
\begin{equation}
\bar{x}=\sum\limits_{j=1}^N \frac{{\left(\delta^2x_j\right)}^{-1}}{\sum\limits_{i=1}^N {\left(\delta^2x_i\right)}^{-1}} x_j,
\label{Eq:Appendix:Mean:meanexpanded}
\end{equation}
with
\begin{equation}
\delta^2\bar{x}=\sum\limits_{j=1}^N \frac{{\left(\delta^2x_j\right)}^{-1}}{\left(\sum\limits_{i=1}^N {\left(\delta^2x_i\right)}^{-1}\right)^2}=\frac{1}{\sum\limits_{i=1}^N {\left(\delta^2x_i\right)}^{-1}}.
\label{Eq:Appendix:Mean:varofmeanexpanded}
\end{equation}

\chapter{The Relationship between the $\Delta U$ Distributions in Forward and Backward TP\label{chapter:Appendix:DeltaUDistributions}}
In a forward TP calculation between $H_0$ and $H_1$, the distribution of $\Delta U$ is
\begin{equation}
    f(\Delta U)=\frac{{\displaystyle\int} e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}{{\displaystyle\int} e^{-\beta H_0}\diff x}.
\end{equation}
While in a backward TP, the distribution is
\begin{align}
    g(\Delta U)=&\frac{\displaystyle\int e^{-\beta H_1}\delta(H_1-H_0-\Delta U)\diff x}{\displaystyle\int e^{-\beta H_1}\diff x}\notag\\
               =&\frac{f(\Delta U)\displaystyle\int e^{-\beta H_1}\delta(H_1-H_0-\Delta U)\diff x\int e^{-\beta H_0}\diff x}{\displaystyle\int e^{-\beta H_1}\diff x\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}\notag\\
               =&f(\Delta U)\frac{\displaystyle\int e^{-\beta H_0}\diff x }{\displaystyle\int e^{-\beta H_1}\diff x}\frac{\displaystyle\int e^{-\beta H_1}\delta(H_1-H_0-\Delta U)\diff x}{\displaystyle\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}\notag\\
               =&f(\Delta U)e^{\beta\Delta A}\frac{\displaystyle\int e^{-\beta\Delta H}e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}{\displaystyle\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}\notag\\
               =&f(\Delta U)e^{\beta\Delta A}\frac{e^{-\beta\Delta U}\displaystyle\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}{\displaystyle\int e^{-\beta H_0}\delta(H_1-H_0-\Delta U)\diff x}\notag\\
               =&f(\Delta U)e^{\beta\Delta A}e^{-\beta\Delta U}
\end{align}
Or it can be written as
\begin{equation}
    g(\Delta U)e^{\beta\Delta U}=f(\Delta U)e^{\beta\Delta A},
\end{equation}
which was first shown by Shing and Gubbins for Widom insertion and deletion\cite{ShingMolPhys1982}.

By looking at this equation and the integral in energy space for TP (Eq.~\ref{Eq:FEM:TP:deltaA7}), 
\begin{equation}
	\Delta A = -\frac{1}{\beta} \ln \int \exp(-\beta \Delta U) f(\Delta U) \diff\Delta U,
\end{equation}
it can be easily realized that the integrand is proportional to the probability of energy difference sampled from state 1 ($g(\Delta U)$) and the free energy difference $\Delta A$ can be estimated reliably only when the sampling at state $0$ covers the representative configurations of state $1$.

\chapter{Cumulant Expansion for the Free Energy Difference in Thermodynamic Perturbation Calculations\label{chapter:Appendix:CE}}
In Thermodynamic Perturbation, the free energy difference between states are expressed as
\begin{equation}
	\Delta A=-\beta^{-1}\ln{\left<\exp{\left[-\beta \Delta U\right]}\right>}_0.
	\label{Eq:Appendix:CE:Exp}
\end{equation}
Due to the exponential term on the right-hand-side of this equation, the convergence is usually slow. This expression can be expanded into Taylor series, and the leading terms can converge much faster than the complete sum.

Using the Taylor expansion for $e^x=1+\sum\limits_{n=1}^{\infty}\frac{1}{n!}x^n$ first, we have
\begin{equation}
	\exp{\left[-\beta \Delta U\right]}=1+(-\beta \Delta U)+\frac{1}{2!}(-\beta \delta U)^2+\frac{1}{3!}(-\beta \Delta U)^3+\cdots.
\end{equation}
Then Eq.~\ref{Eq:Appendix:CE:Exp} becomes
\begin{equation}
	\Delta A=-\beta^{-1}\ln{\left(1-\beta\left<\Delta U\right>_0+\frac{1}{2}\beta^2\left<\Delta U^2\right>_0-\frac{1}{6}\beta^3\left<\Delta U^3\right>_0+\cdots\right)}.
\end{equation}
Using the Taylor expansion for $\ln{(1+x)}=x-\frac{1}{2}x^2+\frac{2}{3!}x^3-\frac{6}{4!}x^4+\cdots$, we have
\begin{align}
	\Delta A=&\left<\Delta U\right>_0-\frac{1}{2}\beta\left<\Delta U^2\right>_0+\frac{1}{6}\beta^2\left<\Delta U^3\right>_0+\cdots\notag\\
	         &+\frac{1}{2}\left(\beta\left<\Delta U\right>_0^2-\beta^2\left<\Delta U\right>_0\left<\Delta U^2\right>_0+\frac{1}{4}\beta^3\left<\Delta U^2\right>_0^2+\cdots\right)\notag\\
	         &-\frac{1}{3}\left(-\beta^2\left<\Delta U\right>_0^3+\cdots\right)\notag\\
	         &+\cdots\notag\\
	        =&\left<\Delta U\right>_0-\frac{\beta}{2}\left(\left<\Delta U^2\right>_0-\left<\Delta U\right>_0^2\right)\notag\\
	         &+\frac{\beta^2}{6}\left(\left<\Delta U^3\right>_0-3\left<\Delta U^2\right>_0\left<\Delta U\right>_0+2\left<\Delta U\right>_0^3\right)+\cdots
\end{align}


\chapter{MBAR Returns to BAR When Only Two States Are Considered\label{chapter:Appendix:twostatMBAR2BAR}}
When there are only two states, the free energy in Eq.~\ref{Eq:FEM:MBAR:f_i_final} for the 1st state in MBAR becomes
\begin{align}
f_1=&-{\beta_1}^{-1}\ln{\sum\limits_{n=1}^N \frac{\exp{\left(-\beta_1 U_1(\mathbf{R}_n)\right)}}{\sum_{k=1}^{2}N_k\exp{\left(\beta_kf_k-\beta_kU_k(\mathbf{R}_n)\right)}}}\notag\\
   =&-{\beta_1}^{-1}\ln{\sum\limits_{j=1}^{2}\sum\limits_{n=1}^{N_j} \frac{\exp{\left(-\beta_1 U_1(\mathbf{R}_{jn})\right)}}{\sum_{k=1}^{2}N_k\exp{\left(\beta_kf_k-\beta_kU_k(\mathbf{R}_{jn})\right)}}},
\end{align}
or equivalently we have
\begin{equation}
1=\sum\limits_{n=1}^{N} \frac{\exp{\left(\beta_1 f_1-\beta_1 U_1(\mathbf{R}_{n})\right)}}{N_1\exp{\left(\beta_1f_1-\beta_1U_1(\mathbf{R}_{n})\right)}+N_2\exp{\left(\beta_2f_2-\beta_2U_2(\mathbf{R}_{n})\right)}},
\end{equation}
\begin{align}
N_1=&\sum\limits_{n=1}^{N_1} \frac{1}{1+\frac{N_2}{N_1}\exp{\left(\Delta f-\Delta U(\mathbf{R}_{1n})\right)}}\notag\\
    &+\sum\limits_{n=1}^{N_2} \frac{1}{1+\frac{N_2}{N_1}\exp{\left(\Delta f-\Delta U(\mathbf{R}_{2n})\right)}}\notag\\
\end{align}
where $\Delta f=\beta_2 f_2-\beta_1 f_1$ and $\Delta U=\beta_2 U_2-\beta_1 U_1$. We further define $M=-\ln{\frac{N_2}{N_1}}$, then
\begin{align}    
N_1=&\sum\limits_{n=1}^{N_1} \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{1n})-M\right)}}\notag\\
    &+\sum\limits_{n=1}^{N_2} \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)}}\notag\\
0=&\sum\limits_{n=1}^{N_1}\left[ \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{1n})-M\right)}}-1\right]\notag\\
  &+\sum\limits_{n=1}^{N_2} \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)}}\notag
\end{align}
\begin{equation}
\sum\limits_{n=1}^{N_1}\frac{1}{1+\exp{\left(-\Delta f+\Delta U(\mathbf{R}_{1n})+M\right)}}
=\sum\limits_{n=1}^{N_2} \frac{1}{1+\exp{\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)}}\notag
\end{equation}
\begin{equation}
\sum\limits_{n=1}^{N_1}f\left(-\Delta f+\Delta U(\mathbf{R}_{1n})+M\right)=\sum\limits_{n=1}^{N_2} f\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)\notag
\end{equation}
\begin{equation}
N_1\left<f\left(-\Delta f+\Delta U(\mathbf{R}_{1n})+M\right)\right>_1=N_2\left<f\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)\right>_2\notag
\end{equation}
\begin{equation}
\frac{\left<f\left(\Delta f-\Delta U(\mathbf{R}_{2n})-M\right)\right>_2}{\left<f\left(-\Delta f+\Delta U(\mathbf{R}_{1n})+M\right)\right>_1}=\frac{N_1}{N_2},\notag
\end{equation}
which is Eq.~\ref{Eq:FEM:BAR:BAR}.

\chapter{MBAR is a binless form of WHAM\label{chapter:Appendix:MBARandWHAM}}
Maybe you have already noticed that MBAR and WHAM have very similar forms for the free energy. 
So you may want to ask if there is any connection between MBAR and WHAM. The answer is YES. 
MBAR is a binless form of WHAM.\cite{TanJCP2012} Let us follow Zhang et al\cite{ZhangMS2016} 
and rewrite Eq.~\ref{Eq:FEM:WHAM:f_k_iteration} into an integral form
\begin{equation}
f_i=-\ln\int\Omega\exp{(-\beta_iU)}\diff U.
\label{Eq:Appendix:MBARandWHAM:f_integral}
\end{equation}
Taking Eq.~\ref{Eq:FEM:WHAM:Omega_iteration} into Eq.~\ref{Eq:Appendix:MBARandWHAM:f_integral}, we find
\begin{equation}
f_i=-\ln\int\frac{\sum\limits_{k=1}^{K}H_k(U)\exp{(-\beta_iU)}}{\sum\limits_{k=1}^{K}N_k\exp{(f_k-\beta_kU)}}\diff U,
\label{Eq:Appendix:MBARandWHAM:f_integral_whole}
\end{equation}
where ${g_{mk}}^{-1}$ has been omitted and $H_{mk}$ has been changed to continuous form $H_k(U)$. From the definition,
\begin{equation}
H_k(U)=\sum\limits_{\mathbf{R}}^{(k)}\delta (U(\mathbf{R})-U).
\label{Eq:Appendix:MBARandWHAM:H_k}
\end{equation}
Taking Eq.~\ref{Eq:Appendix:MBARandWHAM:H_k} into Eq.~\ref{Eq:Appendix:MBARandWHAM:f_integral_whole}, we have
\begin{align}
f_i=&-\ln\sum\limits_{k=1}^{K}\sum\limits_{\mathbf{R}}^{(k)}\frac{\exp{(-\beta_iU(\mathbf{R}))}}{\sum\limits_{k=1}^{K}N_k\exp{(f_k-\beta_kU(\mathbf{R}))}}\notag\\
   =&-\ln\sum\limits_{n=1}^{N}\frac{\exp{(-\beta_iU_i(\mathbf{R_n}))}}{\sum\limits_{k=1}^{K}N_k\exp{(f_k-\beta_kU_k(\mathbf{R_n}))}},
\end{align}
which is Eq.~\ref{Eq:FEM:MBAR:f_i_final}.

\chapter{Jensen's inequality\label{chapter:Appendix:JI}}
\begin{equation}
	e^{\left<X\right>}\leq \left<e^X\right>
\end{equation}


\chapter{Bias-variance decomposition\label{chapter:Appendix:BVD}}
Given a set of samples, the mean squared error (M.S.E.) of an estimator $\hat{\theta}$ for some parameter $\theta$ is
\begin{equation}
	\text{M.S.E.} = \mathbb{E}{\left[\left(\hat{\theta}-\theta\right)^2\right]}.
\end{equation}
It can be decomposed into a bias term and a variance term as following
\begin{align}
	\text{M.S.E.} =& \mathbb{E}{\left[\left(\hat{\theta}-\theta\right)^2\right]}\notag\\
	       =& \mathbb{E}{\left[\left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}+\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)^2\right]}\notag\\
	       =& \mathbb{E}{\left[ \left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}\right)^2+ 2\left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}\right)\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)  +\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)^2\right]}\notag\\
	       =&\mathbb{E}{\left[ \left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}\right)^2\right]}+\mathbb{E}{\left[\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)^2\right]}\notag\\
	       =& \operatorname{Var}+\operatorname{Bias}^2,
\end{align}
where we have utilized 
\begin{equation}
	\mathbb{E}{\left[\left(\hat{\theta}-\mathbb{E}{\left(\hat{\theta}\right)}\right)\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)\right]}=\left(\mathbb{E}\left(\hat{\theta}\right)-\mathbb{E}{\left(\hat{\theta}\right)}\right)\left(\mathbb{E}{\left(\hat{\theta}\right)}-\theta\right)=0,
\end{equation}
since $\mathbb{E}(\hat{\theta})$ and $\theta$ are constants.

%\addtocontents{toc}{\endgroup}
\end{appendices}